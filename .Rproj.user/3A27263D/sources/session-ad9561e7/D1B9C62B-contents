---
title: "Practical Machine Learning Final Project"
author: "Rodolfo Leon"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This is Courseraâ€™s Practical Machine Learning course Final Project Report.

## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: (http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

## Objective
The project requires that a prediction be made of the manner in which the experiments subjects performed the exercises. Different models will be tested to select the one with the higher level of accuracy and apply it to the predetermined test data set as well as to the twent y exercises of the final quiz.

## Environment preparation, install required packages
```{r, message=FALSE}
library(tidyverse)
library(ggplot2)
library(lattice)
library(caret)  
library(kernlab)
library(rattle)
library(randomForest)  
library(corrplot) 
library(Amelia)
library(datasets)
library(caTools)
library(party)
library(dplyr)
library(magrittr)
library(xgboost)
```

## Load Data and replace #DIV/0 elements and empty strings witn NA.
```{r}
train <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                          na.strings=c('#DIV/0', '', 'NA') ,stringsAsFactors = FALSE)
test <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                          na.strings= c('#DIV/0', '', 'NA'),stringsAsFactors = FALSE)
dim(train)
dim(test)
```
#### Keep original datasets to preserve data integrity
```{r}
train1 <- train
test1 <- test
```

#### Check for missing values and zero variance predictors
```{r}
missmap(train, main = "Missing values map - train")
```

There is a substantial number of missing data that must be removed.

#### 1) Remove NA elements
```{r}
rmNA <- (colSums(is.na(train1)) == 0)
train1 <- train1[, rmNA]
dim(train1)
```

#### 2) Remove Near Zero Variables - NZV
```{r}
nzv <- nearZeroVar(train1)
train1 <- train1[,-nzv]
dim(train1)
```

#### 3) Remove columns with variables not relevant to the study
```{r}
train1 <- train1[,-c(1:7)]
dim(train1)
```

## Create and validate the models
Split the clean train1 dataset 70/30 into training and validating datasets. 
The test1 dataset provided for the exercise will not be used until the end. 
The model with the highest accuracy will be applied to the final quiz exercises.  
```{r}
inTrain <- createDataPartition(y=train1$classe, p=0.7, list=FALSE)
trainMod <- train1[inTrain,]
validMod <- train1[-inTrain,]
```

We will test several models, namely Decision Trees, Random Forests, 
Gradient Boosted Trees and Support Vector Machine.
We will use a 3-fold cross validation control with the training model.

```{r}
control <- trainControl(method="cv", number=3, verboseIter=FALSE)
```

### 1. Decision Tree
```{r, fig.width = 10, fig.heigth = 6.7}
decTree <- train(classe~., data=trainMod, method="rpart", trControl = control, tuneLength = 5)
fancyRpartPlot(decTree$finalModel)
```

```{r}
treePred <- predict(decTree, validMod)
conf_trees <- confusionMatrix(treePred, factor(validMod$classe))
conf_trees
```

### 2. Random Forest
```{r}
rfMod <- train(classe~., data=trainMod, method="rf", trControl = control, tuneLength = 5)
rfPred <- predict(rfMod, validMod)
rfConf <- confusionMatrix(rfPred, factor(validMod$classe))
rfConf
```

### 3. Gradient Boosted Trees - GBM
```{r}
gbmMod <- train(classe~., data=trainMod, method="gbm", trControl = control, tuneLength = 5, verbose = FALSE)
gbmPred <- predict(gbmMod, validMod)
gbmConf <- confusionMatrix(gbmPred, factor(validMod$classe))
gbmConf
```

### 4. Support Vector Machine - SVM
```{r}
svmMod <- train(classe~., data=trainMod, method="svmLinear", trControl = control, tuneLength = 5, verbose = FALSE)
svmPred <- predict(svmMod, validMod)
svmConf <- confusionMatrix(svmPred, factor(validMod$classe))
svmConf
```

### Accuracy Results

| Model | Accuracy | OoS Error|
|:-----:|:--------:|:---------|
|RF     | 0.9947   |0.0053    |
|GBM    | 0.9922   |0.0078    |
|SVM    | 0.7721   |0.2279    |
|Tree   | 0.5601   |0.43      |

Quite visibly, Random Forest returned the best results with 0.9929 accuracy, 
so we will use it to predict the outcome on the original test set = 'test'.

## Prediction on test set 

```{r}
rfPred <- predict(rfMod, test)
rfPred
```

Plot the Models
```{r}
plot(decTree)
```

```{r}
plot(rfMod)
```



```{r}
plot(gbmMod)
```

